# US Accidents Lakehouse Pipeline

Pipeline de donnÃ©es moderne pour l'analyse des accidents de la route aux Ã‰tats-Unis, implÃ©mentÃ© avec Apache Spark et une architecture lakehouse en couches (Bronze, Silver, Gold).

## ğŸ—ï¸ Architecture

### Architecture Core

Le projet utilise une architecture modulaire basÃ©e sur des classes core :

- **[`Config`](src/core/config.py)** : Gestion centralisÃ©e de la configuration via variables d'environnement
- **[`Logger`](src/core/logger.py)** : SystÃ¨me de logging avec rotation automatique des fichiers
- **[`SparkManager`](src/core/spark_manager.py)** : Gestionnaire singleton pour les sessions Spark

### Structure du Projet

```
â”œâ”€â”€ src/                          # Code source
â”‚   â”œâ”€â”€ core/                     # Classes core
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration centralisÃ©e
â”‚   â”‚   â”œâ”€â”€ logger.py            # SystÃ¨me de logging
â”‚   â”‚   â””â”€â”€ spark_manager.py     # Gestionnaire Spark
â”‚   â”œâ”€â”€ feeder/                  # Bronze Layer (Ingestion)
â”‚   â”‚   â””â”€â”€ feeder_app.py
â”‚   â”œâ”€â”€ preprocessor/            # Silver Layer (Nettoyage)
â”‚   â”‚   â””â”€â”€ preprocessor_app.py
â”‚   â””â”€â”€ utils/                   # Utilitaires
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ docker/                      # Configuration Docker
â”‚   â”œâ”€â”€ Dockerfile              # Image Docker
â”‚   â”œâ”€â”€ docker-compose.yml      # Orchestration
â”‚   â”œâ”€â”€ requirements.txt        # DÃ©pendances Docker
â”‚   â””â”€â”€ .env.docker            # Variables d'environnement Docker
â”œâ”€â”€ config/                     # Configuration
â”‚   â”œâ”€â”€ .env.template          # Template de configuration
â”‚   â””â”€â”€ spark-defaults.conf    # Configuration Spark
â”œâ”€â”€ data/                       # DonnÃ©es d'entrÃ©e
â”œâ”€â”€ output/                     # DonnÃ©es de sortie
â”‚   â”œâ”€â”€ bronze/                # DonnÃ©es brutes
â”‚   â”œâ”€â”€ silver/                # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ gold/                  # DonnÃ©es agrÃ©gÃ©es (Ã  venir)
â”œâ”€â”€ logs/                       # Fichiers de log
â”œâ”€â”€ scripts/                    # Scripts utilitaires
â””â”€â”€ .env                       # Configuration dÃ©veloppement local
```

## ğŸš€ Installation et Configuration

### 1. PrÃ©requis

- Docker et Docker Compose
- Git

### 2. Configuration

1. **Cloner le projet** :
   ```bash
   git clone <repository-url>
   cd tp_final_bdfs
   ```

2. **Configurer l'environnement de dÃ©veloppement** :
   ```bash
   cp config/.env.template .env
   # Ã‰diter .env selon vos besoins
   ```

3. **PrÃ©parer les donnÃ©es** :
   ```bash
   # Placer votre fichier sample.csv dans le dossier data/
   cp your-sample.csv data/sample.csv
   ```

### 3. Variables d'Environnement

Le fichier `.env` contient la configuration principale :

```env
# MySQL Configuration
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=accidents_db
MYSQL_USER=root
MYSQL_PASSWORD=password

# Spark Configuration
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=2
SPARK_UI_ENABLED=true
SPARK_UI_PORT=4040

# Application Configuration
LOG_LEVEL=INFO
BATCH_SIZE=1000
OUTPUT_FORMAT=parquet

# Paths
INPUT_PATH=/app/data/sample.csv
BRONZE_OUTPUT_PATH=/app/output/bronze
SILVER_OUTPUT_PATH=/app/output/silver
```

## ğŸ³ DÃ©ploiement Docker

### Lancement du Pipeline Complet

```bash
# Depuis la racine du projet
./run-pipeline.sh
```

### Lancement par Composant

```bash
# Bronze Layer uniquement
./run-feeder.sh

# Silver Layer uniquement (aprÃ¨s Bronze)
./run-preprocessor.sh
```

### Commandes Docker Manuelles

```bash
# Build et lancement depuis le dossier docker/
cd docker/
docker-compose up --build

# Lancement en arriÃ¨re-plan
docker-compose up -d

# ArrÃªt
docker-compose down
```

## ğŸ“Š Utilisation des Classes Core

### Configuration

```python
from src.core.config import Config

# Chargement automatique du fichier .env
config = Config()

# AccÃ¨s aux variables
db_host = config.get('MYSQL_HOST')
spark_memory = config.get('SPARK_DRIVER_MEMORY', '2g')
```

### Logging

```python
from src.core.logger import Logger

# Initialisation du logger
logger = Logger(__name__)

# Utilisation
logger.info("DÃ©marrage du traitement")
logger.error("Erreur lors du traitement", exc_info=True)
```

### Spark Manager

```python
from src.core.spark_manager import SparkManager

# RÃ©cupÃ©ration de la session Spark (singleton)
spark = SparkManager.get_spark()

# Utilisation normale de Spark
df = spark.read.csv("path/to/file.csv", header=True)
```

## ğŸ” Monitoring et Logs

### Structure des Logs

Les logs sont organisÃ©s par composant et horodatÃ©s :

```
logs/
â”œâ”€â”€ Feeder_Day1_20250621_152630.log
â”œâ”€â”€ Feeder_Day2_20250621_152645.log
â”œâ”€â”€ Preprocessor_Day1_20250621_152700.log
â””â”€â”€ application_20250621.log
```

### Validation du Pipeline

```bash
# VÃ©rification complÃ¨te du pipeline
./scripts/validate_pipeline.sh
```

### AccÃ¨s aux Logs

```bash
# Logs en temps rÃ©el
tail -f logs/application_$(date +%Y%m%d).log

# Recherche dans les logs
grep "ERROR" logs/*.log
```

## ğŸƒâ€â™‚ï¸ DÃ©veloppement

### DÃ©veloppement Local

1. **Installation des dÃ©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

2. **Configuration** :
   ```bash
   cp config/.env.template .env
   # Adapter les chemins pour le dÃ©veloppement local
   ```

3. **Lancement direct** :
   ```bash
   python src/feeder/feeder_app.py --date 2025-01-01
   python src/preprocessor/preprocessor_app.py --date 2025-01-01
   ```

### Tests

```bash
# Test des classes core
python test_core.py

# Validation de la configuration
python -c "from src.core.config import Config; print(Config().get_all())"
```

## ğŸ“ˆ Pipeline de DonnÃ©es

### Bronze Layer (Feeder)

- **Objectif** : Ingestion des donnÃ©es brutes
- **Format d'entrÃ©e** : CSV
- **Format de sortie** : Parquet partitionnÃ© par date
- **Traitement** : Validation basique, partitionnement temporel

### Silver Layer (Preprocessor)

- **Objectif** : Nettoyage et standardisation
- **Format d'entrÃ©e** : Parquet (Bronze)
- **Format de sortie** : Parquet nettoyÃ©
- **Traitement** : 
  - Nettoyage des donnÃ©es manquantes
  - Standardisation des formats
  - Validation de la qualitÃ©
  - SÃ©paration en datasets thÃ©matiques

### Gold Layer (Ã€ venir)

- **Objectif** : AgrÃ©gations et mÃ©triques business
- **Format d'entrÃ©e** : Parquet (Silver)
- **Format de sortie** : Tables optimisÃ©es pour l'analyse

## ğŸ”§ DÃ©pendances

### Production

```
pyspark==3.5.0
numpy==1.26.0
pyarrow==15.0.0
findspark==2.0.1
python-dotenv==1.0.0
pymysql==1.1.0
sqlalchemy==2.0.25
```

### Remarques

- **Pandas supprimÃ©** : RemplacÃ© par Spark DataFrame pour de meilleures performances
- **Architecture modulaire** : Classes core rÃ©utilisables
- **Configuration centralisÃ©e** : Variables d'environnement
- **Logging avancÃ©** : Rotation automatique des fichiers

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes Courants

1. **Erreur de mÃ©moire Spark** :
   ```bash
   # RÃ©duire la mÃ©moire dans .env
   SPARK_DRIVER_MEMORY=2g
   SPARK_EXECUTOR_MEMORY=2g
   ```

2. **Fichier .env non trouvÃ©** :
   ```bash
   cp config/.env.template .env
   ```

3. **Permissions Docker** :
   ```bash
   # VÃ©rifier les permissions des dossiers
   chmod -R 755 output/ logs/
   ```

4. **Logs non gÃ©nÃ©rÃ©s** :
   ```bash
   # CrÃ©er le dossier logs
   mkdir -p logs
   chmod 777 logs
   ```

### Validation de l'Installation

```bash
# Test complet
./scripts/validate_pipeline.sh

# Test des classes core
python -c "
from src.core.config import Config
from src.core.logger import Logger
from src.core.spark_manager import SparkManager

print('âœ“ Config OK')
print('âœ“ Logger OK') 
print('âœ“ SparkManager OK')
"
```

## ğŸ“ Changelog

### Version 2.0 (Actuelle)

- âœ… Architecture core avec classes Config, Logger, SparkManager
- âœ… Suppression de pandas
- âœ… RÃ©organisation Docker dans dossier dÃ©diÃ©
- âœ… SystÃ¨me de logging avancÃ© avec rotation
- âœ… Configuration centralisÃ©e par variables d'environnement
- âœ… Scripts shell mis Ã  jour pour Docker

### Version 1.0

- âœ… Pipeline Bronze/Silver fonctionnel
- âœ… Support Docker de base
- âœ… Traitement par batch

## ğŸ¤ Contribution

1. Fork du projet
2. CrÃ©ation d'une branche feature
3. Tests des modifications
4. Pull request avec description dÃ©taillÃ©e

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.